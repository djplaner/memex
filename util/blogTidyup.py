"""
blogTidyup.py

Modify the ~/docs/blog folder structure (containing output of Wordpress to markdown) to better fit the old blog structure. May end up working from one folder (outside memex, created by Wordpress to markdown), filtering and writing any changes to new (memex) location

Work to be done

- Modify any old links to davidtjones.wordpress.com
- Fix up folder structure for blog posts
    YYYY/MM/<slug> to YYYY/MM/DD/<slug>
- Adding comments
    - Including the parent/child relationship
- Move pages from date based folder structure to one based on URLs (matching Wordpress blog)

"""

import os
import shutil
import glob
import markdown
import re
import urllib.request
from datetime import datetime

from wpparser import parse
from pprint import pprint

BROKEN_LINKS = {}

OUTDATED_CONTENT = [
    {
        "pattern": r"\\\[googlevideo=[^\]]*\\\]",
        "replace": "Google video no longer available"
    },
]

# Location of XML files from Wordpress
WORDPRESS_EXPORT_FILE = "/Users/davidjones/Downloads/export.xml"
# Location of markdown files generated by Wordpress to markdown - contains pages and posts folders
INPUT_FOLDER = "/Users/davidjones/blog/output"
OUTPUT_FOLDER = "/Users/davidjones/memex/docs/blog"

OLD_BLOG_URL="https://davidtjones.wordpress.com"
CURRENT_BLOG_URL = "https://djon.es/blog"

def readBlogMarkdown(markdownFile):
    """
    Return the markdown content and frontmatter content for a markdown file given by full path

    returns a dict containing the following keys
    - content - the markdown content (minus YAML front matter)
    - yaml - the YAML front matter (as a dict)
    """

    md = markdown.Markdown(extensions = ['meta'])
    pageData = {}
    with open(markdownFile, encoding="utf-8-sig") as f:
        pageData["content"] = f.read()
        html = md.convert(pageData["content"])
        pageData['yaml'] = md.Meta
        pageData['html'] = html

        for key in pageData['yaml'].keys():
            pageData['yaml'][key] = pageData['yaml'][key][0]
            # remove any quotes surrounding the value
            pageData['yaml'][key] = pageData['yaml'][key].lstrip('\"').rstrip('\"')

            #-- remove the yaml frontmatter from pageData['content']
            pattern = r"^---\n.*---\n"
            pageData['content'] = re.sub(pattern, "", pageData['content'], flags=re.DOTALL) 

        return pageData

    return None

def isBrokenLink( link ):
    """
    Check any valid full URL (http/https) to see if it's broken
    """
    #-- ensure it's a full URL - starting with http 
    if not link.startswith("http"):
        return False
    
    req = urllib.request.Request(link)

    try:
        resp = urllib.request.urlopen(req)
        if resp.status in [400,404,403,408,409,501,502,503]:
            print(f"Broken link: {link} - {resp.status}")
            return True
    except urllib.error.HTTPError as e:
        print(f"Broken link: {link} - {e.code}")
        return True

    return False

def cleanupContent(content):
    """
    Given markdown from a blog post/page, perform the following cleanup

    - Replace any old blog links
    - ?? check for broken links
    """

    #-- create dict containing all links extracted from content
    # - key will be original link found in content
    # - value will be what to replace it with
    links = {}
    #-- extract all links from content (markdown)
    matchRe = r'\[.*?\]\((.*?)\)'
    matches = re.findall(matchRe, content)
    # convert matches to dict
    links = {match: match for match in matches}

    for link in links.keys():
        #-- check if link is an old blog link
        if OLD_BLOG_URL in link:
            #-- replace with current blog URL
            newLink = link.replace(OLD_BLOG_URL, CURRENT_BLOG_URL)
            links[link] = newLink
            #print(f"Link: {link} -> {newLink}")

            # replace the link in the content
            content = content.replace(link, newLink)

        # if link not in BROKEN_LINKS check if broken link
#        if link not in BROKEN_LINKS and isBrokenLink(link):
#            BROKEN_LINKS[link] = True

    #-- replace any outdated content
    for outdated in OUTDATED_CONTENT:
        print(f"SSSSSSSs {outdated['pattern']} -> {outdated['replace']}")
        content = re.sub(outdated["pattern"], f"""
!!! warning "Outdated content no longer available"

    {outdated["replace"]}\n""", content)
        
    return content


def updatePages(xml):
    """
    Perform all updates required for Blog pages

    - Move from date to old blog file structure
    - Look for old links to wordpress.com and update
    - Look for other broken links??

    Also create the dummy pages.md file with an index of all pages added

    Process
    - Get all names of pages
    - for each page
        - Read the markdown file - frontmatter and markdown
        - Find a matching page in the XML
        - Create a folder using the page location from XML
        - if there's an images folder, copy it and contents across
        - Parse content
            - Find any links to old blog posts and update them
            - Find any broken links
    """

    # get full paths for the markdown files for all pages
    pageMarkdownFiles = glob.glob(f"{INPUT_FOLDER}/pages/*/*/*/index.md")

    pagesAdded = []

    for page in pageMarkdownFiles:
        #----------- get content for this page
        # - pageData - yaml and content (markdown) from exported markdown file
        # - pageXmlData - raw content from XML export file

        #- get the markdown content and frontmatter
        pageData = readBlogMarkdown(page)
        if pageData is None:
            raise Exception(f"Error reading {page}")

#        pageData['content'] = cleanupContent(pageData['content'])
        title = pageData['yaml']['title']
        #- find the post in the XML
        pageXmlData = findXmlPost( xml, title, "page" )
        if pageXmlData is None:
            print(f"Error: No matching page found for {title} in XML...skipping")
            continue

        #---------------------- perform various cleanups on the content
        #-- clean up the markdown content
        pageData['content'] = cleanupContent(pageData['content'])

        print("--" * 40)
        print(f"Title: {pageXmlData['title']} type {pageXmlData['post_type']} status {pageXmlData['status']}")
        print(f"post_date {pageXmlData['post_date']}")
        print(f"post_link {pageXmlData['link']}")
#        print(pageData['content'])

        updateMemexFolder(page, pageData, pageXmlData)

        #-- update pagesAdded with sub-folders that the page was copied to local to docs
        # - pageXmlData['link'] without the CURRENT_BLOG_URL
        pageAdd = pageXmlData['link'].replace(CURRENT_BLOG_URL, '')
        #pageAdd = page.replace("index.md", "").split("/")[-2]
        print(f")))))))) page: {page} >>> Page added: {pageAdd}")
        pagesAdded.append( pageAdd )

        #input("Press Enter to continue...")

    writePagesIndex(pagesAdded)

def writePagesIndex(pages):
    """
    Create a temp page.md file in the blog folder with a list of all pages added
    """

    #-- write the new index.md file
    with open(f"{OUTPUT_FOLDER}/pages.md", "w", encoding="utf-8") as f:
        f.write("---\n")
        f.write("title: Pages\n")
        f.write("type: pages\n")
        f.write("---\n")
        #-- add in some additional pre-amble
        f.write(f"\nSee also: [[blog-home]], [[pages]], [[posts]]\n\n")
        #-- write the content
        for page in pages:
            #-- get the last part of the path for the name
            name = page.split("/")[-2]
            #-- remove any leading / from page
            page = page.lstrip("/")
            f.write(f"- [{name}]({page})\n")

        
def updateMemexFolder(page : str, pageData, pageXmlData):
    """
    Copy content into memex for a new post/page.
    - Create a new folder for the page in memex
    - Create an images folder within it iff not exists
    - Create an index.md file in the folder with updated content 

    Parameters
    page: full path to the old markdown file
    pageData: dict containing the content and frontmatter (from original markdown file)
    pageXmlData: dict containing the XML data for the page
    """

    sourcePath = f"{OUTPUT_FOLDER}/{pageXmlData['link'].replace(CURRENT_BLOG_URL, '')}"
    destinationPath = page.replace("index.md", "")
    print(f"$$$$$$ New Folder path: {sourcePath} old folderpath {destinationPath}" )

    #-- create the new folder if it doesn't exist 
    if not os.path.exists(sourcePath):
        os.makedirs(sourcePath)
    if os.path.exists(f"{destinationPath}/images"):
        os.makedirs(f"{sourcePath}/images", exist_ok=True)
        #-- copy the images folder contents
        shutil.copytree(f"{destinationPath}/images", f"{sourcePath}/images", dirs_exist_ok=True)

    #-- write the new index.md file
    with open(f"{sourcePath}/index.md", "w", encoding="utf-8") as f:
        #-- write the frontmatter
        f.write("---\n")
        for key in pageData['yaml'].keys():
            if key == "author":
                continue
            if key == "title" and ":" in pageData['yaml'][key]:
                pageData['yaml'][key] = f'"{pageData['yaml'][key]}"';
            f.write(f"{key}: {pageData['yaml'][key]}\n")
        #-- add in memex frontmatter
        f.write(f"type: {pageXmlData['post_type']}\n")
        f.write("---\n")
        #----------- add in some additional pre-amble
        # add metadata for the post (date, tags, etc)
        metaData = generateMetaDataMarkdown(pageXmlData)
        f.write(f"\n{metaData}\n")

        #- see also links
        f.write(f"\nSee also: [[blog-home]]\n")
        #-- write the content
        f.write(pageData['content'])

def generateMetaDataMarkdown(pageXmlData):
    """
    Convert XML data about a post into formatted markdown to be shown at the top of a post/page

    Showing at least post_date, tags, categories
    """

    dateObject = datetime.strptime( pageXmlData['post_date'], "%Y-%m-%d %H:%M:%S")
    postDateMd = f"**Post date:** {dateObject.strftime("%A, %B %d, %Y %I:%M %p")}\n"
    categoryMd = ""
    if 'categories' in pageXmlData and len(pageXmlData['categories']) > 0:
        categoryMd = f"    **Categories:** {', '.join( str(x) for x in pageXmlData['categories'])}\n"
    ## create string tags by joining with commas
    tagMd = ""
    if 'tags' in pageXmlData and len(pageXmlData['tags']) > 0:
        tagMd = f"\    **Tags:** {', '.join( str(x) for x in pageXmlData['tags'])} \n"

    return f"""
!!! info inline end ""

    {postDateMd}{categoryMd}{tagMd}
"""

def findXmlPost( xml, title, type="page"):
    """
    Return the Wordpress XML post matching the given title and type, but only if the post is published

    params:
    xml - the parsed XML data
    title - the title of the post to find
    type - the post type to find (page, post, attachment)
    """

    for post in xml["posts"]:
        if post['status'] != "publish":
            continue
        if post['post_type'] == type and post['title'] == title:
            return post

    return None

def showPosts(data):
    """
    Dump out posts - test function

    data["posts"] contain all items, including attachment,post,page,
    """

    for post in data["posts"]:
        if post['status'] != "publish":
            continue
        if post["post_type"] != "page":
            continue
        if post["post_type"] == "attachment":
            continue

#        if len(post["comments"])==0:
#            continue
#        print(post['content'])
        print(f"Title: {post['title']} type {post['post_type']} status {post['status']}")
        print(f"Link: {post['link']}")
#        pprint(post["comments"])
        print("-" * 40)
#        input("Press Enter to continue...")

def reportOutcomes():
    """
    Called at completion to give a summary of operations
    """

    #-- identify broken links
    # - not actually doing this check
#    print(f"Found {len(BROKEN_LINKS)} broken links")
#    pprint(BROKEN_LINKS)

if __name__ == "__main__":


    wordpressXml = parse(WORDPRESS_EXPORT_FILE)
#    showPosts(wordpressXml)

#    showPosts(wordpressXml)

    updatePages(wordpressXml)

    reportOutcomes()

