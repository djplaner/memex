---
title: The algorithm everyone thinks they understand
type: note
tags:
    - AI
    - llms
image: https://upload.wikimedia.org/wikipedia/commons/3/38/Martian_face_viking.jpg
---

What follows is a [summary](#summary) of and [thoughts](#thoughts) arising from [The Algorithm Everyone Thinks They Understand](https://chrisbigum.blogspot.com/2025/12/bibs-bobs-30.html) by Chris Bigum. Chris has much more to say on the question of LLMs and higher education's response in earlier posts (see [Semiotics and Agency in LLM Education, a wee manifesto](https://chrisbigum.blogspot.com/2025/12/bibs-bobs-29.html)).

TLDR

- What GenAI is, is less important than which interpretations of GenAI are being stablised, and who benefits from those interpretations.

## Summary

Chris ends the post with

> So the real question for higher education isn’t: “What is GenAI, really?” It’s: “Which interpretations are we choosing to stabilise—and who benefits from those choices?”

This is preceded by an observation on higher education's (and education more generally) response to previous disruptions (e.g. calculator, Wikipedia etc): "ban first and then crudely domesticate". An observation he's been making for 20+ years, and which still applies. A description likely to apply to the response to LLMs.

But, Chris suggests that LLMs are different because they appear "to have become __the most productive meaning-generating machine universities have ever seen__. However, LLMs are not generating any agreed meaning". Instead, different academics (and others) examining LLMs given different cosmologies, not evaluations. Largely due to the interest laden perspectives.

### Interest laden perspectives

Two broad groups are the "skeptics" (LLMs are stochastic parrots etc) and the "enthusiasts" (a productivity miracle etc). As you move to specific purposes/experts, different interpretations emerge:

- Plagiarism - assessment experts (and others);
- Personal tutors for all - educational futurists;
- Efficiency (smaller salary bill) - university management; etc.

This might point to mass [pareidolia](https://en.wikipedia.org/wiki/Pareidolia) - a tendency to impose a meaningful interpretation on a nebulous stimulus. Can you see the face in the image of the Martian surface below?

<figure markdown>
![Viking 1, NASA, Public domain, via Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/3/38/Martian_face_viking.jpg)
<caption>Viking 1, NASA, Public domain, via Wikimedia Commons</caption>
</figure>

But Chris argues that rather than pareidolia, these interpretations are "__interest__-laden readings".

Further he argues that the lack of settlement is getting worse as our understanding of GenAI increases. New capabilities create new boundary disputes, each update destabilises previous certainty.

### The Rorschach machine problem

Chris argues that LLMs being more like a __capacity redistribution device_ is a key cause/challenge to how institutions respond. LLMs redistribute capacity by shifting: "Who can write, Who can code;..."

This shifting of capacity causes categories to become unstable. This results in less than stellar institutional responses.

### The real question

The suggestion is that "What is GenAI, really?" is the wrong question. Rather it's: "Which interpretations are we choosing to stabilise—and who benefits from those choices?"

!!! note "A question which CoPilot predicted"

    I wrote this using the VsCode editor with [CoPilot installed](https://code.visualstudio.com/docs/copilot/overview). My main engagement with LLMs. CoPilot correctly suggested an auto-complete for Chris's key question above.

    My assumption is that it didn't automatically retrieve Chris's post from the link above and factor the content into its suggestions to me. Instead, it just came up with the question on its own.


## Thoughts

I agree that the focus on which interpretations are being stablised, and who benefits are very important questions. However, I don't think that figuring out what GenAI is, isn't also an important (but not quite so important) question. At the very least it's not an entirely separate question. Instead, the nature of GenAI will influence which interpretations are stabilised, and who benefits. But also having a better understanding of GenAI will help improve some of those interpretations and figuring out if and how to work around problematic interpretations.

### GenAI through Arthur's definition of technology

Chris's key question reminds me of [Arthur's definition of technology](https://www.the-vital-edge.com/the-nature-of-technology/) (see also [[drons-technology]] for more) as "the __orchestration__ of __phenomena__ for some __purpose__". This lens sparks different questions that appear to fit with both of Chris's questions.

Who's doing the orchestration? For what (and whose) purpose? Questions that echo Chris's main question.  

What are the phenomena being orchestrated? How does the nature and limitations of those phenomena influence how easily and effectively they can be orchestrated to achieve the specific purpose(s)? For me, this echoes the "What is GenAI, really?" question.

### Is GenAI a "good" capacity redistribution device?

GenAI certainly appears to be a capacity redistribution device, but is it really any good? [This toot](https://indieweb.social/@remixtures@tldr.nettime.org/115940108023617654) reports on research into GenAI coding and its impact on productivity. A slowly emerging consensus amongst the small amount of cited research suggests "AI tools don't create talent as much as they amplify it" and "AI's primary role in software development is that of amplifier".

This echoes the experience of my friends and I. GenAI has certainly sped up some of the tasks I've faced whilst coding, but it's problems still require observation and correction by someone who knows what they're doing. Non-technical colleagues hyped about the promise of vibe coding have been disappointed by the results. Primarily due to the difficulty of orchestrating GenAI to achieve the desired purpose.

So how are students successfully orchestrating GenAI to generate the vast concern within higher education around academic misconduct and assessment? At least part of the answer is provided by [Dave Snowden's take](https://jimruttshow.blubrry.net/the-jim-rutt-show-transcripts/transcript-of-episode-11-dave-snowden/) (search for _halfway_ in that page) on whether or not AI which he argues (emphasis added)

> the problem at the moment is they may exceed us in intelligence, because we’re currently working on meeting them halfway. _We’re reducing human intelligence to rigid processes and structured approaches_, and AI will always be better at that than humans. 

Typical university assessments are perhaps prime examples of "reducing human intelligence to rigid processes and structured approaches". Standardised assessment templates, methods, processes, and marking rubrics employed in massive enrolment courses marked by hordes of under-paid casual markers working to strict-standardised marking times.

So it's not surprising that students might be able to successfully orchestrate GenAI to achieve the desired purpose of passing assessments. But anecdotal evidence (personal observation and reports from colleagues and friends) suggests that the students "success" in orchestration is mainly due to the limitations in the assessment regime, rather than the capabilities of GenAI or the students to orchestrate it effectively.


[//begin]: # "Autogenerated link references for markdown compatibility"
[drons-technology]: ../nodt/drons-technology "Dron's take on technology"
[//end]: # "Autogenerated link references"