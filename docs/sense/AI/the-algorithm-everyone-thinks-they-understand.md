---
title: The algorithm everyone thinks they understand
type: note
tags:
    - AI
    - llms
image: https://upload.wikimedia.org/wikipedia/commons/3/38/Martian_face_viking.jpg
---

Summary and thoughts from [The Algorithm Everyone Thinks They Understand](https://chrisbigum.blogspot.com/2025/12/bibs-bobs-30.html) by Chris Bigum.

## Summary

Chris ends the post with

> So the real question for higher education isn’t: “What is GenAI, really?” It’s: “Which interpretations are we choosing to stabilise—and who benefits from those choices?”

Chris starts by describing how higher education has adopted previous disruptions (calculator, Wikipedia etc): "ban first and then crudely domesticate". An observation he's been making for 20+ years, and which still applies.

He suggests LLMs are different because they appear "to have become __the most productive meaning-generating machine universities have ever seen__, without generating any agreed meaning". Instead, different academics examining LLMs given different cosmologies, not evaluations.

### Interest laden perspectives

Two broad groups are the "skeptics" (LLMs are stochastic parrots etc) and the "enthusiasts" (a productivity miracle etc). As you move to specific purposes/experts, different interpretations emerge:

- Plagiarism - assessment experts (and others);
- Personal tutors for all - educational futurists;
- Efficiency (smaller salary bill) - university management; etc.

This might point to mass [pareidolia](https://en.wikipedia.org/wiki/Pareidolia) - a tendency to impose a meaningful interpretation on a nebulous stimulus. Can you see the face in the image of the Martian surface below?

<figure markdown>
![Viking 1, NASA, Public domain, via Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/3/38/Martian_face_viking.jpg)
<caption>Viking 1, NASA, Public domain, via Wikimedia Commons</caption>
</figure>

But Chris argues that rather than pareidolia, these interpretations are "__interest__-laden readings".

Further he argues that the lack of settlement is getting worse as our understanding of GenAI increases. New capabilities create new boundary disputes, each update destabilises previous certainty.

### The Rorschach machine problem

Chris argues that LLMs being more like a __capacity redistribution device_ is a key cause/challenge to how institutions respond. LLMs redistribute capacity by shifting: "Who can write, Who can code;..."

This shifting of capacity causes categories to become unstable. This results in less than stellar institutional responses.

### The real question

The suggestion is that "What is GenAI, really?" is the wrong question. Rather it's: "Which interpretations are we choosing to stabilise—and who benefits from those choices?"

!!! note "A question which CoPilot predicted"

    I wrote this using the VsCode editor with [CoPilot installed](https://code.visualstudio.com/docs/copilot/overview). My main engagement with LLMs. CoPilot correctly suggested an auto-complete for Chris's key question above.

    My assumption is that it didn't automatically retrieve Chris's post from the link above and factor the content into its suggestions to me. Instead, it just came up with the question on its own.


## Thoughts

### GenAIs through Arthur's definition of technology

Chris's key question reminds me of Arthur's definition of technology (see [[drons-technology]] for more) as "the __orchestration__ of __phenomena__ for some __purpose__".

Who's doing the orchestration? For what (and whose) purpose? 

But that definition also has me wondering about the phenomena. Which potentially connects with what Chris thinks is the wrong question. What is GenAI? What are the phenomena it offers up for orchestration? What can it orchestrate?



- Are LLMs a radically different beast? Why? How does it compare to the Internet?

[//begin]: # "Autogenerated link references for markdown compatibility"
[drons-technology]: ../nodt/drons-technology "Dron's take on technology"
[//end]: # "Autogenerated link references"